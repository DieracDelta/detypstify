@article{alexander1994template,
  title={Template-based algorithms for connectionist rule extraction},
  author={Alexander, Jay and Mozer, Michael C},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@book{bower2012book,
  title={The book of GENESIS: exploring realistic neural models with the GEneral NEural SImulation System},
  author={Bower, James M and Beeman, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{hasselmo1995dynamics,
  title={Dynamics of learning and recall at excitatory recurrent synapses and cholinergic modulation in rat hippocampal region CA3},
  author={Hasselmo, Michael E and Schnell, Eric and Barkai, Edi},
  journal={Journal of Neuroscience},
  volume={15},
  number={7},
  pages={5249--5262},
  year={1995},
  publisher={Soc Neuroscience}
}

@misc{finetuning-good1,
      title={Universal Language Model Fine-tuning for Text Classification}, 
      author={Jeremy Howard and Sebastian Ruder},
      year={2018},
      eprint={1801.06146},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{finetuning-good2,
      title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey}, 
      author={Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
      year={2024},
      eprint={2403.14608},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{trocr, title={TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models}, volume={37}, url={https:/
/ojs.aaai.org/index.php/AAAI/article/view/26538}, DOI={10.1609/aaai.v37i11.26538}, abstractNote={Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu}, year={2023}, month={Jun.}, pages={13094-13102} }

@misc{dataset,
  title={IM2LATEX-230k},
  howpublished={\url{https://www.kaggle.com/datasets/gregoryeritsyan/im2latex-230k/data}},
}

@misc{typst-dataset,
  title={im2typst-230k},
  howpublished={\url{https://www.kaggle.com/datasets/jachymp/im2typst-230k/data}},
}



@misc{openai,
  title={Requests for Research},
  author={OpenAI},
  url={https://github.com/openai/requests-for-research/blob/master/_requests_for_research/im2latex.html}
}

@misc{latex_ocr,
  title={LaTeX OCR},
  author={Lukas Blecher},
  url={https://github.com/lukas-blecher/LaTeX-OCR/tree/main}
}
